{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is frfom TF learning boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length of the training data 3000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "comments     object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the input data files, tab seperated values\n",
    "data_file = glob('/tf/deep_learning/sentiment_analysis/data/*.txt')\n",
    "model_file = '/tf/deep_learning/sentiment_analysis/lib/model'\n",
    "header_list = [\"comments\", \"sentiment\"]\n",
    "# read all the data using windows encoding and python engine. Else it will give error for windows files\n",
    "l = [pd.read_csv(f, sep='\\t', names=header_list, encoding = \"ISO-8859-1\", engine='python') for f in data_file]\n",
    "data = pd.concat(l, axis=0)\n",
    "print('total length of the training data %s'%(len(data)))\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(2400,)\n",
      "(450,)\n",
      "(450,)\n",
      "(150,)\n",
      "(150,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get training data, use balanced split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"comments\"], data[\"sentiment\"], test_size=0.20, random_state=42, stratify=data['sentiment'])\n",
    "# get test and validation data, use balanced split\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.25, random_state=42, stratify=y_test)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "# drop the base data and free the memory\n",
    "del [[l,data]]\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96    The scenes are often funny and occasionally to...\n",
      "Name: comments, dtype: object\n",
      "96    1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:1])\n",
    "print(y_train[:1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1200\n",
      "0    1200\n",
      "Name: sentiment, dtype: int64\n",
      "1    225\n",
      "0    225\n",
      "Name: sentiment, dtype: int64\n",
      "1    75\n",
      "0    75\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check if the data is balanced\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())\n",
    "print(y_val.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization layer that lowercases text, splits on whitespace, \n",
    "# strips punctuation, and outputs integer vocab indices\n",
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 100  # Sequence length to pad the outputs to.\n",
    "embedding_dims = 16\n",
    "# Create the layer.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len)\n",
    "\n",
    "vectorize_layer.adapt(np.array(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4728\n"
     ]
    }
   ],
   "source": [
    "# check total voab\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 96    The scenes are often funny and occasionally touching as the characters evaluate their lives and where they are going.  \n",
      "Name: comments, dtype: object\n",
      "Label 96    1\n",
      "Name: sentiment, dtype: int64\n",
      "Vectorized review tf.Tensor(\n",
      "[[   2  261   25  671  203    3 1046 1284   27    2  145 3870   90 1522\n",
      "     3  220   50   25  155    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]], shape=(1, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a review from the dataset\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "first_review, first_label = X_train[:1], y_train[:1]\n",
    "print(\"Review\", first_review)\n",
    "print(\"Label\", first_label)\n",
    "print(\"Vectorized review\", vectorize_text(first_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --->  b'the'\n",
      "261 --->  b'scenes'\n"
     ]
    }
   ],
   "source": [
    "# according to documentations the metrix is off by 2\n",
    "# Note that this vocabulary contains 1 OOV token, \n",
    "# so the effective number of tokens is (max_tokens - 1 - (1 if output == \"int\" else 0)).\n",
    "print(\"2 ---> \",vectorize_layer.get_vocabulary()[2-2])\n",
    "print(\"261 ---> \",vectorize_layer.get_vocabulary()[261-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v = np.array(vectorize_text(X_train))\n",
    "X_test_v = np.array(vectorize_text(X_test))\n",
    "X_val_v = np.array(vectorize_text(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,  261,   25, ...,    0,    0,    0],\n",
       "       [  49,  266, 1020, ...,    0,    0,    0],\n",
       "       [  13,  255,   15, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 151,  675,    0, ...,    0,    0,    0],\n",
       "       [  51,  325,    0, ...,    0,    0,    0],\n",
       "       [1734,  340,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          80016     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 80,033\n",
      "Trainable params: 80,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dims),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1, activation='linear')])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveBestModel = tf.keras.callbacks.ModelCheckpoint(filepath='/tf/deep_learning/sentiment_analysis/my_model.h5',monitor='binary_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq=1)\n",
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath='/tf/deep_learning/sentiment_analysis/my_model.h5', save_best_only=True)\n",
    "earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2400/2400 - 9s - loss: 0.6918 - binary_accuracy: 0.5254 - val_loss: 0.6874 - val_binary_accuracy: 0.6467\n",
      "Epoch 2/20\n",
      "2400/2400 - 9s - loss: 0.6771 - binary_accuracy: 0.6275 - val_loss: 0.6694 - val_binary_accuracy: 0.7467\n",
      "Epoch 3/20\n",
      "2400/2400 - 9s - loss: 0.6400 - binary_accuracy: 0.7371 - val_loss: 0.6391 - val_binary_accuracy: 0.6667\n",
      "Epoch 4/20\n",
      "2400/2400 - 9s - loss: 0.5852 - binary_accuracy: 0.7804 - val_loss: 0.5931 - val_binary_accuracy: 0.7467\n",
      "Epoch 5/20\n",
      "2400/2400 - 9s - loss: 0.5223 - binary_accuracy: 0.8179 - val_loss: 0.5533 - val_binary_accuracy: 0.7667\n",
      "Epoch 6/20\n",
      "2400/2400 - 9s - loss: 0.4647 - binary_accuracy: 0.8392 - val_loss: 0.5250 - val_binary_accuracy: 0.7067\n",
      "Epoch 7/20\n",
      "2400/2400 - 9s - loss: 0.4111 - binary_accuracy: 0.8604 - val_loss: 0.4901 - val_binary_accuracy: 0.8133\n",
      "Epoch 8/20\n",
      "2400/2400 - 9s - loss: 0.3650 - binary_accuracy: 0.8846 - val_loss: 0.4743 - val_binary_accuracy: 0.7867\n",
      "Epoch 9/20\n",
      "2400/2400 - 9s - loss: 0.3234 - binary_accuracy: 0.8988 - val_loss: 0.4419 - val_binary_accuracy: 0.8333\n",
      "Epoch 10/20\n",
      "2400/2400 - 9s - loss: 0.2936 - binary_accuracy: 0.9112 - val_loss: 0.4237 - val_binary_accuracy: 0.8333\n",
      "Epoch 11/20\n",
      "2400/2400 - 9s - loss: 0.2648 - binary_accuracy: 0.9258 - val_loss: 0.4218 - val_binary_accuracy: 0.8200\n",
      "Epoch 12/20\n",
      "2400/2400 - 9s - loss: 0.2423 - binary_accuracy: 0.9250 - val_loss: 0.4386 - val_binary_accuracy: 0.7933\n",
      "Epoch 13/20\n",
      "2400/2400 - 9s - loss: 0.2241 - binary_accuracy: 0.9367 - val_loss: 0.3962 - val_binary_accuracy: 0.8267\n",
      "Epoch 14/20\n",
      "2400/2400 - 9s - loss: 0.2042 - binary_accuracy: 0.9413 - val_loss: 0.4061 - val_binary_accuracy: 0.8133\n",
      "Epoch 15/20\n",
      "2400/2400 - 9s - loss: 0.1883 - binary_accuracy: 0.9492 - val_loss: 0.4054 - val_binary_accuracy: 0.8133\n",
      "Epoch 16/20\n",
      "2400/2400 - 9s - loss: 0.1782 - binary_accuracy: 0.9467 - val_loss: 0.3944 - val_binary_accuracy: 0.8333\n",
      "Epoch 17/20\n",
      "2400/2400 - 9s - loss: 0.1623 - binary_accuracy: 0.9558 - val_loss: 0.3944 - val_binary_accuracy: 0.8200\n",
      "Epoch 18/20\n",
      "2400/2400 - 9s - loss: 0.1531 - binary_accuracy: 0.9600 - val_loss: 0.4089 - val_binary_accuracy: 0.8333\n",
      "Epoch 19/20\n",
      "2400/2400 - 9s - loss: 0.1436 - binary_accuracy: 0.9608 - val_loss: 0.4058 - val_binary_accuracy: 0.8400\n",
      "Epoch 20/20\n",
      "2400/2400 - 9s - loss: 0.1321 - binary_accuracy: 0.9671 - val_loss: 0.4098 - val_binary_accuracy: 0.8267\n"
     ]
    }
   ],
   "source": [
    "training_batch=1\n",
    "epochs = 20\n",
    "history = model.fit(\n",
    "    X_train_v, y_train, \n",
    "    validation_data=(X_val_v, y_val),\n",
    "    batch_size=training_batch,\n",
    "    epochs=epochs,callbacks=[checkpoint_cb, earlystopping_cb], verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4438 - binary_accuracy: 0.8044\n",
      "Loss:  0.44380539655685425\n",
      "Accuracy:  0.804444432258606\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_v, y_test)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makig model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorize_layer)\n",
    "model.add(tf.keras.layers.Embedding(max_features+1, embedding_dims))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_6 (TextVe (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 16)           160016    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 16)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
